{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "455e78ae-532c-40cc-ba34-ca0dac8f86cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "#  Strictly Leak-Free Meta-Model Pipeline  (AI-Cup 2025 🏓)\n",
    "#  – builds swing-level LightGBM models with GroupKFold\n",
    "#  – trains both: (a) full-data meta model, and (b) 9-fold ensemble meta model\n",
    "#  – generates 2 separate competition submissions\n",
    "# ================================================================\n",
    "\n",
    "import numpy as np, pandas as pd, lightgbm as lgb, warnings\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import GroupKFold, train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. Feature Extraction\n",
    "DATA_DIR = Path(\"39_Training_Dataset\")\n",
    "INFO_CSV = DATA_DIR / \"train_info.csv\"\n",
    "TXT_DIR  = DATA_DIR / \"train_data\"\n",
    "\n",
    "def extract_features_from_swing(a: np.ndarray) -> dict:\n",
    "    Ax,Ay,Az = a[:,0],a[:,1],a[:,2]\n",
    "    Gx,Gy,Gz = a[:,3],a[:,4],a[:,5]\n",
    "    acc  = np.linalg.norm(a[:,:3],1)\n",
    "    gyro = np.linalg.norm(a[:,3:],1)\n",
    "    return dict(\n",
    "        Ax_mean=Ax.mean(), Ax_std=Ax.std(),\n",
    "        Ay_mean=Ay.mean(), Ay_std=Ay.std(),\n",
    "        Az_mean=Az.mean(), Az_std=Az.std(),\n",
    "        Gx_mean=Gx.mean(), Gx_std=Gx.std(),\n",
    "        Gy_mean=Gy.mean(), Gy_std=Gy.std(),\n",
    "        Gz_mean=Gz.mean(), Gz_std=Gz.std(),\n",
    "        acc_mag_mean =acc.mean(),\n",
    "        gyro_mag_mean=gyro.mean(),\n",
    "        acc_vs_gyro_ratio=acc.mean()/(gyro.mean()+1e-6)\n",
    "    )\n",
    "\n",
    "def build_dataset(txt_dir: Path, info_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for txt in tqdm(sorted(txt_dir.glob(\"*.txt\")), desc=\"Extract swings\"):\n",
    "        fid = int(txt.stem)\n",
    "        meta = info_df[info_df['unique_id']==fid].iloc[0]\n",
    "        cps = np.fromstring(meta['cut_point'].strip('[]'), sep=' ', dtype=int)\n",
    "        if len(cps) < 2: continue\n",
    "        data = np.loadtxt(txt, skiprows=1)\n",
    "        for i in range(len(cps)-1):\n",
    "            swing = data[cps[i]:cps[i+1]]\n",
    "            d = extract_features_from_swing(swing)\n",
    "            d.update(file_id=fid, swing_id=i,\n",
    "                     gender=meta['gender'],\n",
    "                     handed=meta['hold racket handed'],\n",
    "                     years=meta['play years'],\n",
    "                     level=meta['level'])\n",
    "            rows.append(d)\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1d041cf-e5fc-47e4-8d82-249407568a39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extract swings: 100%|██████████████████████████████████████████████████████████████| 1955/1955 [00:29<00:00, 65.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset (52785, 21)\n"
     ]
    }
   ],
   "source": [
    "info_df = pd.read_csv(INFO_CSV)\n",
    "df = build_dataset(TXT_DIR, info_df)\n",
    "print(\"Dataset\", df.shape)\n",
    "\n",
    "# 2. Treat ALL data as train (no holdout split)\n",
    "df['split'] = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d67df85c-d416-4163-b650-97b80f407a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. OOF Model Helpers\n",
    "def get_oof_models(df, label, le, multiclass=False):\n",
    "    drop = ['file_id','swing_id','split','gender','handed','years','level',label]\n",
    "    X = df.drop(columns=drop)\n",
    "    y = le.transform(df[label])\n",
    "    gid = df['file_id'].values\n",
    "    boosters, meta_models = [], []\n",
    "    oof_dict = {}\n",
    "\n",
    "    for tr, va in GroupKFold(9).split(X, y, gid):\n",
    "        dtr, dva = lgb.Dataset(X.iloc[tr], label=y[tr]), lgb.Dataset(X.iloc[va], label=y[va])\n",
    "        booster = lgb.train(\n",
    "            dict(objective='multiclass' if multiclass else 'binary',\n",
    "                 metric='multi_logloss' if multiclass else 'auc',\n",
    "                 num_class=len(le.classes_) if multiclass else 1,\n",
    "                 learning_rate=0.05, verbosity=-1, seed=42),\n",
    "            dtr, num_boost_round=1000, valid_sets=[dva], callbacks=[lgb.early_stopping(50)]\n",
    "        )\n",
    "        boosters.append(booster)\n",
    "        probs = booster.predict(X.iloc[va])\n",
    "        for fid, p in zip(df.iloc[va]['file_id'].values, probs):\n",
    "            oof_dict.setdefault(fid, []).append(p)\n",
    "    return boosters, oof_dict\n",
    "\n",
    "def convert_to_meta_feats(preds_per_file, df_all, label, multiclass):\n",
    "    feats, y, groups = [], [], []\n",
    "    for fid, swings in preds_per_file.items():\n",
    "        swings = np.array(swings)\n",
    "        if not multiclass:\n",
    "            swings = swings.reshape(-1,1)\n",
    "        feats.append(np.concatenate([\n",
    "            swings.mean(0), swings.max(0), swings[swings.sum(1).argmax()]\n",
    "        ]))\n",
    "        y.append(df_all[df_all['file_id']==fid][label].iloc[0])\n",
    "        groups.append(fid)\n",
    "    return np.vstack(feats), np.array(y), np.array(groups)\n",
    "\n",
    "def meta_feats_from_booster(booster, swings_df, multiclass):\n",
    "    probs = booster.predict(swings_df)\n",
    "    if not multiclass:\n",
    "        probs = probs.reshape(-1,1)\n",
    "    return np.concatenate([probs.mean(0), probs.max(0), probs[probs.sum(1).argmax()]])\n",
    "\n",
    "def predict_ensemble_meta(boosters, meta_models, test_df, label, le, multiclass):\n",
    "    feats = []\n",
    "    for fid, grp in test_df.groupby(\"file_id\"):\n",
    "        pred_list = [meta_feats_from_booster(b, grp.drop(columns=['file_id','swing_id']), multiclass) for b in boosters]\n",
    "        X = np.vstack(pred_list)\n",
    "        X_mean = X.mean(axis=0).reshape(1,-1)\n",
    "        proba = np.mean([m.predict_proba(X_mean) for m in meta_models], axis=0)\n",
    "        feats.append((fid, proba))\n",
    "    return dict(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73c6e1a-51f5-440c-878a-fb19a797eec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⏳ Training: gender\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[499]\tvalid_0's auc: 0.981392\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[399]\tvalid_0's auc: 0.980582\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[268]\tvalid_0's auc: 0.973297\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[998]\tvalid_0's auc: 0.982325\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[998]\tvalid_0's auc: 0.984025\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[999]\tvalid_0's auc: 0.981611\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[942]\tvalid_0's auc: 0.986017\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[816]\tvalid_0's auc: 0.982855\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[624]\tvalid_0's auc: 0.983328\n",
      "\n",
      "⏳ Training: handed\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[293]\tvalid_0's auc: 0.999892\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[304]\tvalid_0's auc: 0.999956\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[404]\tvalid_0's auc: 0.999918\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[465]\tvalid_0's auc: 0.999896\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[540]\tvalid_0's auc: 0.999844\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[396]\tvalid_0's auc: 0.999889\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[151]\tvalid_0's auc: 0.9998\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[628]\tvalid_0's auc: 0.999926\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[338]\tvalid_0's auc: 0.999967\n",
      "\n",
      "⏳ Training: years\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[878]\tvalid_0's multi_logloss: 0.260117\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[856]\tvalid_0's multi_logloss: 0.282066\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    }
   ],
   "source": [
    "# 4. Train both full model and ensemble\n",
    "labels_cfg = {'gender':False, 'handed':False, 'years':True, 'level':True}\n",
    "meta_models = {}\n",
    "full_models = {}\n",
    "ensemble_meta = {}\n",
    "ensemble_boost = {}\n",
    "\n",
    "for label, multiclass in labels_cfg.items():\n",
    "    print(f\"\\n⏳ Training: {label}\")\n",
    "    df[label] = df[label].astype(str)\n",
    "    le = LabelEncoder().fit(df[label])\n",
    "\n",
    "    boosters, oof_dict = get_oof_models(df, label, le, multiclass)\n",
    "    X_meta, y_meta, g_meta = convert_to_meta_feats(oof_dict, df, label, multiclass)\n",
    "\n",
    "    metas = []\n",
    "    for tr, va in GroupKFold(9).split(X_meta, y_meta, g_meta):\n",
    "        clf = LogisticRegressionCV(max_iter=1000, multi_class='multinomial', cv=3)\n",
    "        clf.fit(X_meta[tr], y_meta[tr])\n",
    "        metas.append(clf)\n",
    "    ensemble_boost[label] = boosters\n",
    "    ensemble_meta[label]  = metas\n",
    "\n",
    "    final_meta = LogisticRegressionCV(max_iter=1000, multi_class='multinomial', cv=5)\n",
    "    final_meta.fit(X_meta, y_meta)\n",
    "    meta_models[label] = final_meta\n",
    "\n",
    "    drop = ['file_id','swing_id','split','gender','handed','years','level',label]\n",
    "    booster_full = lgb.train(\n",
    "        dict(objective='multiclass' if multiclass else 'binary',\n",
    "             metric='multi_logloss' if multiclass else 'auc',\n",
    "             num_class=len(le.classes_) if multiclass else 1,\n",
    "             learning_rate=0.05, verbosity=-1, seed=42),\n",
    "        lgb.Dataset(df.drop(columns=drop), label=le.transform(df[label])),\n",
    "        num_boost_round=800\n",
    "    )\n",
    "    full_models[label] = booster_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aa156b-a6f1-40ce-ab5c-e7c14b7109a2",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b79b097-87a4-4856-bae7-abe3b2e9a237",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 5. Submission Generation\n",
    "TEST_DATA_DIR = Path(\"39_Test_Dataset\")\n",
    "test_info     = pd.read_csv(TEST_DATA_DIR / \"test_info.csv\")\n",
    "test_txt_dir  = TEST_DATA_DIR / \"test_data\"\n",
    "\n",
    "def build_test_df(txt_dir, info_df):\n",
    "    rows = []\n",
    "    for fid in tqdm(info_df['unique_id'].values, desc=\"Loading test swings\"):\n",
    "        path = txt_dir / f\"{fid}.txt\"\n",
    "        if not path.exists(): continue\n",
    "        cps = np.fromstring(info_df[info_df['unique_id']==fid]['cut_point'].values[0].strip('[]'), sep=' ', dtype=int)\n",
    "        if len(cps) < 2: continue\n",
    "        data = np.loadtxt(path, skiprows=1)\n",
    "        for i in range(len(cps)-1):\n",
    "            swing = data[cps[i]:cps[i+1]]\n",
    "            d = extract_features_from_swing(swing)\n",
    "            d.update(file_id=fid, swing_id=i)\n",
    "            rows.append(d)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df_test = build_test_df(test_txt_dir, test_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e65c3af-0cc5-4ec3-b302-ea73cb8f3b9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Submission a: using full-model trained on all data\n",
    "sub_full = pd.read_csv(TEST_DATA_DIR / \"sample_submission.csv\")\n",
    "for label, multiclass in labels_cfg.items():\n",
    "    le = LabelEncoder().fit(df[label].astype(str))\n",
    "    feats = [meta_feats_from_booster(full_models[label],\n",
    "             grp.drop(columns=['file_id','swing_id']), multiclass)\n",
    "             for _, grp in df_test.groupby(\"file_id\")]\n",
    "    prob = meta_models[label].predict_proba(np.vstack(feats))\n",
    "    if multiclass:\n",
    "        for i, cls in enumerate(le.classes_):\n",
    "            sub_full[f\"{label}_{cls}\"] = prob[:, i]\n",
    "    else:\n",
    "        sub_full[f\"{label}_{le.classes_[1]}\"] = prob[:, 1]\n",
    "sub_full.to_csv(f\"submission_meta_full_{datetime.now().strftime('%H%M%S')}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7b04e9-8fc5-451c-a95d-fb8c8622b0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission b: average of ensemble predictions\n",
    "sub_avg = pd.read_csv(TEST_DATA_DIR / \"sample_submission.csv\")\n",
    "for label, multiclass in labels_cfg.items():\n",
    "    le = LabelEncoder().fit(df[label].astype(str))\n",
    "    preds = predict_ensemble_meta(ensemble_boost[label], ensemble_meta[label], df_test, label, le, multiclass)\n",
    "    prob = np.vstack([preds[fid] for fid in test_info['unique_id'].values])\n",
    "    if multiclass:\n",
    "        for i, cls in enumerate(le.classes_):\n",
    "            sub_avg[f\"{label}_{cls}\"] = prob[:, i]\n",
    "    else:\n",
    "        sub_avg[f\"{label}_{le.classes_[1]}\"] = prob[:, 1]\n",
    "sub_avg.to_csv(f\"submission_meta_ensemble_{datetime.now().strftime('%H%M%S')}.csv\", index=False)\n",
    "\n",
    "print(\"✅ Both submissions saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2334f7c2-b6f3-41ee-9c7b-746b9e854271",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645f044f-e061-48b1-99f7-dc81b3aedb83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05d016b-55be-4289-a4c7-365a1f9130ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d48f576-3a1e-4fea-ada1-7c49c17bb91b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1e46ba-d9bb-4778-9e1a-80f51559f549",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AIcup_2025)",
   "language": "python",
   "name": "aicup_2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
