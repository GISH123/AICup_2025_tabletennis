{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40143aa3-01be-4683-898c-086012fd229d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ train_info shape : (1955, 8)\n",
      "✅ No NaNs in metadata\n",
      "\n",
      "⏳ Reading IMU traces …\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "010560fa695146798dbb78646c5b01f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1955 [00:00<?, ?file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1955 traces  (len min/mean/max → 271/2469.1/4695)\n",
      "\n",
      "Sample metadata row:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>player_id</th>\n",
       "      <th>mode</th>\n",
       "      <th>gender</th>\n",
       "      <th>hold racket handed</th>\n",
       "      <th>play years</th>\n",
       "      <th>level</th>\n",
       "      <th>cut_point</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>[   0   61  122  183  244  305  366  428  489 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   unique_id  player_id  mode  gender  hold racket handed  play years  level  \\\n",
       "0          1         41     1       1                   1           1      5   \n",
       "\n",
       "                                           cut_point  \n",
       "0  [   0   61  122  183  244  305  366  428  489 ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sensor slice (first 5 rows of first loaded uid):\n",
      "[[ 1923   313    12  -906   321  -994]\n",
      " [ 1929   297   -20  -833   260 -1005]\n",
      " [ 1907   262   -26  -850    33 -1029]\n",
      " [ 1900   247   -22 -1134  -142  -921]\n",
      " [ 1916   201   -36 -1712  -449  -761]]\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Step 1 ▸ DATA READING + CHECKS\n",
    "# ================================\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import warnings, sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ── 1. paths ─────────────────────────────────────────────────────────\n",
    "TRAIN_DIR   = Path(\"./39_Training_Dataset\")\n",
    "INFO_CSV    = TRAIN_DIR / \"train_info.csv\"\n",
    "SENSOR_DIR  = TRAIN_DIR / \"train_data\"\n",
    "\n",
    "assert INFO_CSV.exists(),  f\"❌ {INFO_CSV} not found\"\n",
    "assert SENSOR_DIR.exists(),f\"❌ {SENSOR_DIR} not found\"\n",
    "\n",
    "# ── 2. metadata ──────────────────────────────────────────────────────\n",
    "info_df = pd.read_csv(INFO_CSV)\n",
    "print(f\"✅ train_info shape : {info_df.shape}\")\n",
    "\n",
    "nan_ct = info_df.isna().sum()\n",
    "if nan_ct.any():\n",
    "    print(\"⚠️  NaN counts:\\n\", nan_ct[nan_ct > 0])\n",
    "else:\n",
    "    print(\"✅ No NaNs in metadata\")\n",
    "\n",
    "dup_ct = info_df[\"unique_id\"].duplicated().sum()\n",
    "if dup_ct:\n",
    "    print(f\"⚠️  Found {dup_ct} duplicate unique_id rows → keeping first occurrence\")\n",
    "    info_df = info_df.drop_duplicates(\"unique_id\", keep=\"first\")\n",
    "\n",
    "# ── 3. sensor-file presence check ────────────────────────────────────\n",
    "sensor_files = sorted(SENSOR_DIR.glob(\"*.txt\"))\n",
    "sensor_ids   = {int(fp.stem) for fp in sensor_files}\n",
    "meta_ids     = set(info_df[\"unique_id\"])\n",
    "\n",
    "missing_txt   = sorted(meta_ids  - sensor_ids)\n",
    "orphan_txt    = sorted(sensor_ids - meta_ids)\n",
    "\n",
    "if missing_txt:\n",
    "    print(f\"⚠️  {len(missing_txt)} metadata rows lack a .txt file (ids→ {missing_txt[:5]} …)\")\n",
    "if orphan_txt:\n",
    "    print(f\"⚠️  {len(orphan_txt)} .txt files have no metadata row (ids→ {orphan_txt[:5]} …)\")\n",
    "\n",
    "# ── 4. helper to read one IMU file ───────────────────────────────────\n",
    "def load_sensor_data(fp: Path) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns int-dtype array of shape (T,6)  [Ax Ay Az Gx Gy Gz].\n",
    "    Skips header / blank lines; ignores malformed rows (<6 values).\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    with open(fp, \"r\") as fh:\n",
    "        for idx, line in enumerate(fh):\n",
    "            if idx == 0 or not line.strip():\n",
    "                continue\n",
    "            vals = line.strip().split()\n",
    "            if len(vals) >= 6:\n",
    "                try:\n",
    "                    rows.append([int(v) for v in vals[:6]])\n",
    "                except ValueError:\n",
    "                    # non-integer token → skip row\n",
    "                    continue\n",
    "    return np.asarray(rows, dtype=np.int32)\n",
    "\n",
    "# ── 5. bulk load (with quick statistics) ─────────────────────────────\n",
    "sensor_raw   = {}\n",
    "lengths      = []\n",
    "\n",
    "print(\"\\n⏳ Reading IMU traces …\")\n",
    "for fp in tqdm(sensor_files, unit=\"file\"):\n",
    "    uid = int(fp.stem)\n",
    "    arr = load_sensor_data(fp)\n",
    "    if arr.size == 0:            # empty / unreadable\n",
    "        continue\n",
    "    sensor_raw[uid] = arr\n",
    "    lengths.append(len(arr))\n",
    "\n",
    "# basic stats\n",
    "if lengths:\n",
    "    print(f\"✅ Loaded {len(sensor_raw)} traces  \"\n",
    "          f\"(len min/mean/max → {min(lengths)}/{np.mean(lengths):.1f}/{max(lengths)})\")\n",
    "else:\n",
    "    sys.exit(\"❌ No sensor data loaded – check path or file format!\")\n",
    "\n",
    "# peek a bit\n",
    "print(\"\\nSample metadata row:\")\n",
    "display(info_df.head(1))\n",
    "print(\"Sample sensor slice (first 5 rows of first loaded uid):\")\n",
    "first_uid = next(iter(sensor_raw))\n",
    "print(sensor_raw[first_uid][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "652d8e5f-ec34-4113-b492-1c2ab4a58f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Helpers ▸ equal-27 segmentation + swing features\n",
    "# ================================\n",
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "def segment_27_equal(arr: np.ndarray):\n",
    "    \"\"\"Split a trace into 27 equal-length swings.\"\"\"\n",
    "    idx = np.linspace(0, len(arr), 28, dtype=int)\n",
    "    return [arr[idx[i]:idx[i+1]] for i in range(27)]\n",
    "\n",
    "def safe_stats(col):\n",
    "    mu = col.mean()\n",
    "    sd = col.std(ddof=0)\n",
    "    return mu, sd, col.max(), col.min()\n",
    "\n",
    "# def extract_swing_features(swing: np.ndarray):\n",
    "#     feat = {}\n",
    "#     for i, axis in enumerate([\"Ax\",\"Ay\",\"Az\",\"Gx\",\"Gy\",\"Gz\"]):\n",
    "#         col = swing[:, i]\n",
    "#         mu, sd, mx, mn = safe_stats(col)\n",
    "#         feat[f\"{axis}_mean\"]     = mu\n",
    "#         feat[f\"{axis}_std\"]      = sd\n",
    "#         feat[f\"{axis}_max\"]      = mx\n",
    "#         feat[f\"{axis}_min\"]      = mn\n",
    "#         feat[f\"{axis}_skew\"]     = skew(col)      if len(col) > 2 else 0.0\n",
    "#         feat[f\"{axis}_kurtosis\"] = kurtosis(col)  if len(col) > 3 else 0.0\n",
    "\n",
    "#     acc_mag  = np.linalg.norm(swing[:, :3], axis=1)\n",
    "#     gyro_mag = np.linalg.norm(swing[:, 3:], axis=1)\n",
    "#     feat[\"acc_mag_mean\"]  = acc_mag.mean()\n",
    "#     feat[\"gyro_mag_mean\"] = gyro_mag.mean()\n",
    "\n",
    "#     half = len(swing)//2 or 1\n",
    "#     feat[\"acc_early_vs_late_ratio\"] = (acc_mag[:half].mean()+1e-6)/(acc_mag[half:].mean()+1e-6)\n",
    "#     return feat\n",
    "# ──── 依賴 ────\n",
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis, entropy\n",
    "\n",
    "# ──── 新版 swing 特徵 ────\n",
    "def extract_swing_features(swing: np.ndarray) -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    時域 + 頻域(FFT) 特徵 for ONE swing\n",
    "    ------------------------------------------------------------\n",
    "    * 時域：mean / std / max / min / skew / kurtosis（與舊版相同）\n",
    "    * 頻域：三段 PSD 能量、頻譜熵、第一諧波位置\n",
    "      - 三段頻帶 = 0–1/3、1/3–2/3、2/3–Nyquist（以 rfft bin 切三等分）\n",
    "    ------------------------------------------------------------\n",
    "    `swing` shape: (N_sample, 6)\n",
    "    \"\"\"\n",
    "    feat: dict[str, float] = {}\n",
    "\n",
    "    # ---------- 時域 ----------\n",
    "    for i, axis in enumerate([\"Ax\",\"Ay\",\"Az\",\"Gx\",\"Gy\",\"Gz\"]):\n",
    "        col = swing[:, i].astype(np.float32)\n",
    "        mu  = col.mean(); sd = col.std(ddof=0)\n",
    "        feat[f\"{axis}_mean\"] = mu\n",
    "        feat[f\"{axis}_std\"]  = sd\n",
    "        feat[f\"{axis}_max\"]  = col.max()\n",
    "        feat[f\"{axis}_min\"]  = col.min()\n",
    "        # 需要長度>2 才能算 skew/kurt\n",
    "        feat[f\"{axis}_skew\"]      = skew(col)      if len(col) > 2 else 0.0\n",
    "        feat[f\"{axis}_kurtosis\"]  = kurtosis(col)  if len(col) > 3 else 0.0\n",
    "\n",
    "    # magnitude（用來計算能量 & entropy）\n",
    "    acc_norm  = np.linalg.norm(swing[:, :3], axis=1).astype(np.float32)\n",
    "    gyro_norm = np.linalg.norm(swing[:, 3:], axis=1).astype(np.float32)\n",
    "\n",
    "    # ---------- 頻域 ----------\n",
    "    def fft_feats(signal: np.ndarray, prefix: str):\n",
    "        \"\"\"回傳 {prefix_band1, band2, band3, entropy, first_harmonic}\"\"\"\n",
    "        # rfft -> 單邊功率\n",
    "        fft = np.fft.rfft(signal - signal.mean())\n",
    "        psd = (np.abs(fft) ** 2).astype(np.float32)   # Power Spectral Density\n",
    "\n",
    "        tot_energy = psd.sum() + 1e-8\n",
    "        thirds = np.array_split(psd, 3)\n",
    "        for i, band in enumerate(thirds, 1):\n",
    "            feat[f\"{prefix}_band{i}\"] = band.sum() / tot_energy\n",
    "\n",
    "        # 頻譜熵 (normalized)\n",
    "        p_norm = psd / tot_energy\n",
    "        feat[f\"{prefix}_entropy\"] = entropy(p_norm)\n",
    "\n",
    "        # 第一諧波位置（最大非 DC bin 的索引 / 總 bin）\n",
    "        if len(psd) > 1:\n",
    "            first_h_idx = np.argmax(psd[1:]) + 1\n",
    "            feat[f\"{prefix}_harmonic_pos\"] = first_h_idx / len(psd)\n",
    "        else:\n",
    "            feat[f\"{prefix}_harmonic_pos\"] = 0.0\n",
    "\n",
    "    fft_feats(acc_norm,  \"acc_psd\")\n",
    "    fft_feats(gyro_norm, \"gyro_psd\")\n",
    "\n",
    "    # 另外補上一些總能量\n",
    "    feat[\"acc_energy\"]  = (acc_norm ** 2).sum() / len(acc_norm)\n",
    "    feat[\"gyro_energy\"] = (gyro_norm ** 2).sum() / len(gyro_norm)\n",
    "\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6c5eafd-87e7-4883-bdfd-0d0d41b8b219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cdd97b56f7041f3afcd0e6cec54dcad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "⏳ Per-swing features:   0%|          | 0/1955 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ swing-level matrix : (52785, 58)\n",
      "✅ 已將 X_sw/y_sw/SESSION_ID 覆寫為 session-level\n",
      "   session-level X_sw.shape: (1955, 232)\n",
      "   session-level y_sw.shape: (1955, 6)\n"
     ]
    }
   ],
   "source": [
    "# # ================================\n",
    "# # Step 2-S ▸ PER-SWING FEATURE TABLE  (27 rows per session)\n",
    "# # ================================\n",
    "# from tqdm.auto import tqdm\n",
    "# import numpy as np, pandas as pd\n",
    "\n",
    "# swing_rows, swing_meta = [], []     # one row = one swing\n",
    "# SESSION_ID, SWING_ID = [], []\n",
    "\n",
    "# for uid, trace in tqdm(sensor_raw.items(), desc=\"⏳ Per-swing features\"):\n",
    "#     swings = segment_27_equal(trace)\n",
    "#     for s_idx, sw in enumerate(swings):\n",
    "#         feat = extract_swing_features(sw)\n",
    "\n",
    "#         # add 10 mode dummies\n",
    "#         mode = info_df.loc[info_df[\"unique_id\"] == uid, \"mode\"].iat[0]\n",
    "#         for m in range(1, 11):\n",
    "#             feat[f\"mode_{m}\"] = int(mode == m)\n",
    "\n",
    "#         swing_rows.append(feat)\n",
    "#         SESSION_ID.append(uid)\n",
    "#         SWING_ID.append(s_idx)\n",
    "\n",
    "#         meta = info_df.loc[info_df[\"unique_id\"] == uid].iloc[0]\n",
    "#         swing_meta.append(meta[[\"gender\",\"hold racket handed\",\n",
    "#                                 \"play years\",\"level\",\"player_id\"]])\n",
    "\n",
    "# X_sw = pd.DataFrame(swing_rows).astype(np.float32)\n",
    "# y_sw = pd.DataFrame(swing_meta).reset_index(drop=True)\n",
    "# print(\"✅ swing-level matrix :\", X_sw.shape)\n",
    "# ================================\n",
    "# Step 2-S ▸ PER-SWING FEATURE TABLE  (27 rows per session)\n",
    "# ================================\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "swing_rows, swing_meta = [], []     # one row = one swing\n",
    "SESSION_ID, SWING_ID = [], []\n",
    "\n",
    "for uid, trace in tqdm(sensor_raw.items(), desc=\"⏳ Per-swing features\"):\n",
    "    swings = segment_27_equal(trace)\n",
    "    for s_idx, sw in enumerate(swings):\n",
    "        feat = extract_swing_features(sw)\n",
    "\n",
    "        # add 10 mode dummies\n",
    "        mode = info_df.loc[info_df[\"unique_id\"] == uid, \"mode\"].iat[0]\n",
    "        for m in range(1, 11):\n",
    "            feat[f\"mode_{m}\"] = int(mode == m)\n",
    "\n",
    "        swing_rows.append(feat)\n",
    "        SESSION_ID.append(uid)\n",
    "        SWING_ID.append(s_idx)\n",
    "\n",
    "        meta = info_df.loc[info_df[\"unique_id\"] == uid].iloc[0]\n",
    "        swing_meta.append(meta[[\"gender\",\"hold racket handed\",\n",
    "                                \"play years\",\"level\",\"player_id\"]])\n",
    "\n",
    "X_sw = pd.DataFrame(swing_rows).astype(np.float32)\n",
    "y_sw = pd.DataFrame(swing_meta).reset_index(drop=True)\n",
    "print(\"✅ swing-level matrix :\", X_sw.shape)\n",
    "\n",
    "# ======= 從此之後，用相同的 X_sw/y_sw/SESSION_ID 來做 session-level 聚合 =======\n",
    "\n",
    "# 1. 先複製 swing-level 特徵，並把 SESSION_ID 拷貝到新欄位 \"unique_id\"\n",
    "X_sw_with_uid = X_sw.copy()\n",
    "X_sw_with_uid[\"unique_id\"] = SESSION_ID\n",
    "\n",
    "# 2. 定義要對 swing-level 每個欄位做的統計量：mean、std、max、min\n",
    "agg_dict = {col: [\"mean\", \"std\", \"max\", \"min\"] for col in X_sw.columns}\n",
    "\n",
    "# 3. 以 unique_id 分群並做聚合，產生 multi-index 欄位\n",
    "sess_agg = (\n",
    "    X_sw_with_uid\n",
    "    .groupby(\"unique_id\")\n",
    "    .agg(agg_dict)\n",
    ")\n",
    "\n",
    "# 4. 將 multi-index 欄位展平為單層，後綴加 “27”\n",
    "sess_agg.columns = [\n",
    "    f\"{feat}_{stat}27\"\n",
    "    for feat, stat in sess_agg.columns\n",
    "]\n",
    "\n",
    "# 5. 把 unique_id 從 index 變成一般欄位\n",
    "sess_agg = sess_agg.reset_index()   # 現在欄位 = ['unique_id', 'Ax_mean27', 'Ax_std27', …]\n",
    "\n",
    "# 6. 用 y_sw + SESSION_ID 建立 session_labels，再 override 回 y_sw\n",
    "y_sw_with_uid = y_sw.copy()\n",
    "y_sw_with_uid[\"unique_id\"] = SESSION_ID\n",
    "\n",
    "session_labels = (\n",
    "    y_sw_with_uid\n",
    "    .groupby(\"unique_id\")\n",
    "    .first()       # 同一 session 下所有 swing 的 label 都相同，取第一筆即可\n",
    "    .reset_index()\n",
    ")\n",
    "# 現在 session_labels 欄位 = ['unique_id','gender','hold racket handed','play years','level','player_id']\n",
    "\n",
    "# 7. 把 X_sw >< y_sw 直接 override 為 session-level  \n",
    "#    X_sw：捨棄唯一識別欄 unique_id，保留 float32 數值特徵  \n",
    "#    y_sw：保留所有 label 欄（包含 unique_id & player_id + 四個預測欄）\n",
    "X_sw = sess_agg.drop(columns=[\"unique_id\"]).astype(np.float32)\n",
    "y_sw = session_labels.copy()\n",
    "\n",
    "# 8. 如果後續程式需要 SESSION_ID，也把它換成所有 session 的 unique_id 列表\n",
    "SESSION_ID = y_sw[\"unique_id\"].tolist()\n",
    "\n",
    "# 9. SWING_ID 已經不再需要──改成全零或空 list，視後續程式是否會用到\n",
    "SWING_ID = [0] * len(SESSION_ID)\n",
    "\n",
    "print(\"✅ 已將 X_sw/y_sw/SESSION_ID 覆寫為 session-level\")\n",
    "print(\"   session-level X_sw.shape:\", X_sw.shape)\n",
    "print(\"   session-level y_sw.shape:\", y_sw.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62a861cd-af28-475a-912d-9bcc564fcfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 0)  extra quantile-pooled features (helper works for BOTH train & test) --\n",
    "def build_quantile_pool(df_swing):\n",
    "    q_list = [0.10, 0.25, 0.50, 0.75, 0.90]\n",
    "    q = df_swing.groupby(\"unique_id\").quantile(q_list).unstack()\n",
    "    q.columns = [f\"{f}_q{int(qv*100):02d}\" for f, qv in q.columns]\n",
    "    return q.reset_index()\n",
    "\n",
    "if \"acc_mag_mean_q10\" not in sess_agg.columns:          # run once\n",
    "    sess_q  = build_quantile_pool(X_sw_with_uid)         # <-- train swings\n",
    "    sess_agg = sess_agg.merge(sess_q, on=\"unique_id\")\n",
    "    X_sw     = sess_agg.drop(columns=\"unique_id\").astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69998b7e-5767-4515-bac3-9d13805f60d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 5 folds, player-wise, swing rows : 1955\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import numpy as np\n",
    "\n",
    "# ── labels\n",
    "y_enc, enc = {}, {}\n",
    "for c in [\"gender\",\"hold racket handed\",\"play years\",\"level\"]:\n",
    "    le = LabelEncoder().fit(y_sw[c])\n",
    "    y_enc[c] = le.transform(y_sw[c]); enc[c] = le\n",
    "\n",
    "# ── groups  (player_id per swing)\n",
    "groups = y_sw[\"player_id\"].values\n",
    "gkf = list(GroupKFold(n_splits=5).split(X_sw, y_enc[\"gender\"], groups))\n",
    "print(\"✅ 5 folds, player-wise, swing rows :\", len(X_sw))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "197fb596-9b61-4094-b7bc-f1e1973de9ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 fitting swing-level models …\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:on't improve for 100 rounds\n",
      "\n",
      "[489]\tvalid_0's auc: 0.756793\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "\n",
      "[489]\tvalid_0's auc: 0.756793\n",
      "\n",
      "Early stopping, best iteration is:on't improve for 100 rounds\n",
      "\n",
      "[489]\tvalid_0's auc: 0.756793\n",
      "\n",
      "\n",
      "[1]\tvalid_0's auc: 1\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "\n",
      "[489]\tvalid_0's auc: 0.756793\n",
      "\n",
      "\n",
      "[1]\tvalid_0's auc: 1\n",
      "\n",
      "Early stopping, best iteration is:on't improve for 100 rounds\n",
      "\n",
      "[489]\tvalid_0's auc: 0.756793\n",
      "\n",
      "\n",
      "[1]\tvalid_0's auc: 1\n",
      "\n",
      "\n",
      "[132]\tvalid_0's auc: 0.679269\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "\n",
      "[489]\tvalid_0's auc: 0.756793\n",
      "\n",
      "\n",
      "[1]\tvalid_0's auc: 1\n",
      "\n",
      "\n",
      "[132]\tvalid_0's auc: 0.679269\n",
      "\n",
      "Early stopping, best iteration is:on't improve for 100 rounds\n",
      "\n",
      "[489]\tvalid_0's auc: 0.756793\n",
      "\n",
      "\n",
      "[1]\tvalid_0's auc: 1\n",
      "\n",
      "\n",
      "[132]\tvalid_0's auc: 0.679269\n",
      "\n",
      "\n",
      "[121]\tvalid_0's auc: 0.959659\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "\n",
      "[489]\tvalid_0's auc: 0.756793\n",
      "\n",
      "\n",
      "[1]\tvalid_0's auc: 1\n",
      "\n",
      "\n",
      "[132]\tvalid_0's auc: 0.679269\n",
      "\n",
      "\n",
      "[121]\tvalid_0's auc: 0.959659\n",
      "\n",
      "Early stopping, best iteration is:on't improve for 100 rounds\n",
      "\n",
      "[489]\tvalid_0's auc: 0.756793\n",
      "\n",
      "\n",
      "[1]\tvalid_0's auc: 1\n",
      "\n",
      "\n",
      "[132]\tvalid_0's auc: 0.679269\n",
      "\n",
      "\n",
      "[121]\tvalid_0's auc: 0.959659\n",
      "\n",
      "\n",
      "[61]\tvalid_0's auc: 0.992354\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "\n",
      "[489]\tvalid_0's auc: 0.756793\n",
      "\n",
      "\n",
      "[1]\tvalid_0's auc: 1\n",
      "\n",
      "\n",
      "[132]\tvalid_0's auc: 0.679269\n",
      "\n",
      "\n",
      "[121]\tvalid_0's auc: 0.959659\n",
      "\n",
      "\n",
      "[61]\tvalid_0's auc: 0.992354\n",
      "\n",
      "Early stopping, best iteration is:on't improve for 100 rounds\n",
      "\n",
      "[489]\tvalid_0's auc: 0.756793\n",
      "\n",
      "\n",
      "[1]\tvalid_0's auc: 1\n",
      "\n",
      "\n",
      "[132]\tvalid_0's auc: 0.679269\n",
      "\n",
      "\n",
      "[121]\tvalid_0's auc: 0.959659\n",
      "\n",
      "\n",
      "[61]\tvalid_0's auc: 0.992354\n",
      "\n",
      "\n",
      "[1]\tvalid_0's auc: 1\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "\n",
      "[489]\tvalid_0's auc: 0.756793\n",
      "\n",
      "\n",
      "[1]\tvalid_0's auc: 1\n",
      "\n",
      "\n",
      "[132]\tvalid_0's auc: 0.679269\n",
      "\n",
      "\n",
      "[121]\tvalid_0's auc: 0.959659\n",
      "\n",
      "\n",
      "[61]\tvalid_0's auc: 0.992354\n",
      "\n",
      "\n",
      "[1]\tvalid_0's auc: 1\n",
      "\n",
      "Early stopping, best iteration is:on't improve for 100 rounds\n",
      "\n",
      "[489]\tvalid_0's auc: 0.756793\n",
      "\n",
      "\n",
      "[1]\tvalid_0's auc: 1\n",
      "\n",
      "\n",
      "[132]\tvalid_0's auc: 0.679269\n",
      "\n",
      "\n",
      "[121]\tvalid_0's auc: 0.959659\n",
      "\n",
      "\n",
      "[61]\tvalid_0's auc: 0.992354\n",
      "\n",
      "\n",
      "[1]\tvalid_0's auc: 1\n",
      "\n",
      "\n",
      "[1]\tvalid_0's auc: 1\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "\n",
      "[2]\tvalid_0's auc: 0.999914\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:on't improve for 100 rounds\n",
      "\n",
      "[1]\tvalid_0's auc: 1\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "\n",
      "[23]\tvalid_0's auc: 0.997232\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:on't improve for 100 rounds\n",
      "\n",
      "[97]\tvalid_0's multi_logloss: 0.678721\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "\n",
      "[97]\tvalid_0's multi_logloss: 0.678721\n",
      "\n",
      "Early stopping, best iteration is:on't improve for 100 rounds\n",
      "\n",
      "[97]\tvalid_0's multi_logloss: 0.678721\n",
      "\n",
      "\n",
      "[18]\tvalid_0's multi_logloss: 1.5216\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "\n",
      "[97]\tvalid_0's multi_logloss: 0.678721\n",
      "\n",
      "\n",
      "[18]\tvalid_0's multi_logloss: 1.5216\n",
      "\n",
      "Early stopping, best iteration is:on't improve for 100 rounds\n",
      "\n",
      "[97]\tvalid_0's multi_logloss: 0.678721\n",
      "\n",
      "\n",
      "[18]\tvalid_0's multi_logloss: 1.5216\n",
      "\n",
      "\n",
      "[35]\tvalid_0's multi_logloss: 0.717192\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "\n",
      "[97]\tvalid_0's multi_logloss: 0.678721\n",
      "\n",
      "\n",
      "[18]\tvalid_0's multi_logloss: 1.5216\n",
      "\n",
      "\n",
      "[35]\tvalid_0's multi_logloss: 0.717192\n",
      "\n",
      "Early stopping, best iteration is:on't improve for 100 rounds\n",
      "\n",
      "[97]\tvalid_0's multi_logloss: 0.678721\n",
      "\n",
      "\n",
      "[18]\tvalid_0's multi_logloss: 1.5216\n",
      "\n",
      "\n",
      "[35]\tvalid_0's multi_logloss: 0.717192\n",
      "\n",
      "\n",
      "[28]\tvalid_0's multi_logloss: 0.8978\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "\n",
      "[97]\tvalid_0's multi_logloss: 0.678721\n",
      "\n",
      "\n",
      "[18]\tvalid_0's multi_logloss: 1.5216\n",
      "\n",
      "\n",
      "[35]\tvalid_0's multi_logloss: 0.717192\n",
      "\n",
      "\n",
      "[28]\tvalid_0's multi_logloss: 0.8978\n",
      "\n",
      "Early stopping, best iteration is:on't improve for 100 rounds\n",
      "\n",
      "[97]\tvalid_0's multi_logloss: 0.678721\n",
      "\n",
      "\n",
      "[18]\tvalid_0's multi_logloss: 1.5216\n",
      "\n",
      "\n",
      "[35]\tvalid_0's multi_logloss: 0.717192\n",
      "\n",
      "\n",
      "[28]\tvalid_0's multi_logloss: 0.8978\n",
      "\n",
      "\n",
      "[1]\tvalid_0's multi_logloss: 1.41667\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "\n",
      "[97]\tvalid_0's multi_logloss: 0.678721\n",
      "\n",
      "\n",
      "[18]\tvalid_0's multi_logloss: 1.5216\n",
      "\n",
      "\n",
      "[35]\tvalid_0's multi_logloss: 0.717192\n",
      "\n",
      "\n",
      "[28]\tvalid_0's multi_logloss: 0.8978\n",
      "\n",
      "\n",
      "[1]\tvalid_0's multi_logloss: 1.41667\n",
      "\n",
      "Early stopping, best iteration is:on't improve for 100 rounds\n",
      "\n",
      "[97]\tvalid_0's multi_logloss: 0.678721\n",
      "\n",
      "\n",
      "[18]\tvalid_0's multi_logloss: 1.5216\n",
      "\n",
      "\n",
      "[35]\tvalid_0's multi_logloss: 0.717192\n",
      "\n",
      "\n",
      "[28]\tvalid_0's multi_logloss: 0.8978\n",
      "\n",
      "\n",
      "[1]\tvalid_0's multi_logloss: 1.41667\n",
      "\n",
      "\n",
      "[13]\tvalid_0's multi_logloss: 1.18055\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "\n",
      "[97]\tvalid_0's multi_logloss: 0.678721\n",
      "\n",
      "\n",
      "[18]\tvalid_0's multi_logloss: 1.5216\n",
      "\n",
      "\n",
      "[35]\tvalid_0's multi_logloss: 0.717192\n",
      "\n",
      "\n",
      "[28]\tvalid_0's multi_logloss: 0.8978\n",
      "\n",
      "\n",
      "[1]\tvalid_0's multi_logloss: 1.41667\n",
      "\n",
      "\n",
      "[13]\tvalid_0's multi_logloss: 1.18055\n",
      "\n",
      "Early stopping, best iteration is:on't improve for 100 rounds\n",
      "\n",
      "[97]\tvalid_0's multi_logloss: 0.678721\n",
      "\n",
      "\n",
      "[18]\tvalid_0's multi_logloss: 1.5216\n",
      "\n",
      "\n",
      "[35]\tvalid_0's multi_logloss: 0.717192\n",
      "\n",
      "\n",
      "[28]\tvalid_0's multi_logloss: 0.8978\n",
      "\n",
      "\n",
      "[1]\tvalid_0's multi_logloss: 1.41667\n",
      "\n",
      "\n",
      "[13]\tvalid_0's multi_logloss: 1.18055\n",
      "\n",
      "\n",
      "[43]\tvalid_0's multi_logloss: 0.740441\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "\n",
      "[97]\tvalid_0's multi_logloss: 0.678721\n",
      "\n",
      "\n",
      "[18]\tvalid_0's multi_logloss: 1.5216\n",
      "\n",
      "\n",
      "[35]\tvalid_0's multi_logloss: 0.717192\n",
      "\n",
      "\n",
      "[28]\tvalid_0's multi_logloss: 0.8978\n",
      "\n",
      "\n",
      "[1]\tvalid_0's multi_logloss: 1.41667\n",
      "\n",
      "\n",
      "[13]\tvalid_0's multi_logloss: 1.18055\n",
      "\n",
      "\n",
      "[43]\tvalid_0's multi_logloss: 0.740441\n",
      "\n",
      "Early stopping, best iteration is:on't improve for 100 rounds\n",
      "\n",
      "[97]\tvalid_0's multi_logloss: 0.678721\n",
      "\n",
      "\n",
      "[18]\tvalid_0's multi_logloss: 1.5216\n",
      "\n",
      "\n",
      "[35]\tvalid_0's multi_logloss: 0.717192\n",
      "\n",
      "\n",
      "[28]\tvalid_0's multi_logloss: 0.8978\n",
      "\n",
      "\n",
      "[1]\tvalid_0's multi_logloss: 1.41667\n",
      "\n",
      "\n",
      "[13]\tvalid_0's multi_logloss: 1.18055\n",
      "\n",
      "\n",
      "[43]\tvalid_0's multi_logloss: 0.740441\n",
      "\n",
      "\n",
      "[1]\tvalid_0's multi_logloss: 0.964907\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "\n",
      "[97]\tvalid_0's multi_logloss: 0.678721\n",
      "\n",
      "\n",
      "[18]\tvalid_0's multi_logloss: 1.5216\n",
      "\n",
      "\n",
      "[35]\tvalid_0's multi_logloss: 0.717192\n",
      "\n",
      "\n",
      "[28]\tvalid_0's multi_logloss: 0.8978\n",
      "\n",
      "\n",
      "[1]\tvalid_0's multi_logloss: 1.41667\n",
      "\n",
      "\n",
      "[13]\tvalid_0's multi_logloss: 1.18055\n",
      "\n",
      "\n",
      "[43]\tvalid_0's multi_logloss: 0.740441\n",
      "\n",
      "\n",
      "[1]\tvalid_0's multi_logloss: 0.964907\n",
      "\n",
      "Early stopping, best iteration is:on't improve for 100 rounds\n",
      "\n",
      "[97]\tvalid_0's multi_logloss: 0.678721\n",
      "\n",
      "\n",
      "[18]\tvalid_0's multi_logloss: 1.5216\n",
      "\n",
      "\n",
      "[35]\tvalid_0's multi_logloss: 0.717192\n",
      "\n",
      "\n",
      "[28]\tvalid_0's multi_logloss: 0.8978\n",
      "\n",
      "\n",
      "[1]\tvalid_0's multi_logloss: 1.41667\n",
      "\n",
      "\n",
      "[13]\tvalid_0's multi_logloss: 1.18055\n",
      "\n",
      "\n",
      "[43]\tvalid_0's multi_logloss: 0.740441\n",
      "\n",
      "\n",
      "[1]\tvalid_0's multi_logloss: 0.964907\n",
      "\n",
      "\n",
      "[23]\tvalid_0's multi_logloss: 1.14857\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "\n",
      "[97]\tvalid_0's multi_logloss: 0.678721\n",
      "\n",
      "\n",
      "[18]\tvalid_0's multi_logloss: 1.5216\n",
      "\n",
      "\n",
      "[35]\tvalid_0's multi_logloss: 0.717192\n",
      "\n",
      "\n",
      "[28]\tvalid_0's multi_logloss: 0.8978\n",
      "\n",
      "\n",
      "[1]\tvalid_0's multi_logloss: 1.41667\n",
      "\n",
      "\n",
      "[13]\tvalid_0's multi_logloss: 1.18055\n",
      "\n",
      "\n",
      "[43]\tvalid_0's multi_logloss: 0.740441\n",
      "\n",
      "\n",
      "[1]\tvalid_0's multi_logloss: 0.964907\n",
      "\n",
      "\n",
      "[23]\tvalid_0's multi_logloss: 1.14857\n",
      "\n",
      "Early stopping, best iteration is:on't improve for 100 rounds\n",
      "\n",
      "[97]\tvalid_0's multi_logloss: 0.678721\n",
      "\n",
      "\n",
      "[18]\tvalid_0's multi_logloss: 1.5216\n",
      "\n",
      "\n",
      "[35]\tvalid_0's multi_logloss: 0.717192\n",
      "\n",
      "\n",
      "[28]\tvalid_0's multi_logloss: 0.8978\n",
      "\n",
      "\n",
      "[1]\tvalid_0's multi_logloss: 1.41667\n",
      "\n",
      "\n",
      "[13]\tvalid_0's multi_logloss: 1.18055\n",
      "\n",
      "\n",
      "[43]\tvalid_0's multi_logloss: 0.740441\n",
      "\n",
      "\n",
      "[1]\tvalid_0's multi_logloss: 0.964907\n",
      "\n",
      "\n",
      "[23]\tvalid_0's multi_logloss: 1.14857\n",
      "\n",
      "\n",
      "[1]\tvalid_0's multi_logloss: 1.46589\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "def train_swing_target(y, num_class=1, params_extra=None):\n",
    "    params = dict(objective='binary' if num_class==1 else 'multiclass',\n",
    "                  metric='auc' if num_class==1 else 'multi_logloss',\n",
    "                  learning_rate=0.03, num_leaves=63,\n",
    "                  feature_fraction=0.8, bagging_fraction=0.8,device_type = \"gpu\",\n",
    "                  bagging_freq=1, seed=42, verbosity=-1)\n",
    "    if num_class>1: params[\"num_class\"] = num_class\n",
    "    if params_extra: params.update(params_extra)\n",
    "\n",
    "    best = []\n",
    "    for tr,vl in gkf:\n",
    "        mdl = lgb.train(params,\n",
    "                        lgb.Dataset(X_sw.iloc[tr], label=y[tr]),\n",
    "                        2000,\n",
    "                        valid_sets=[lgb.Dataset(X_sw.iloc[vl], label=y[vl])],\n",
    "                        callbacks=[lgb.early_stopping(100)])\n",
    "        best.append(mdl.best_iteration)\n",
    "    params[\"num_boost_round\"] = int(np.mean(best)*1.1)\n",
    "    return lgb.train(params, lgb.Dataset(X_sw, label=y))\n",
    "\n",
    "print(\"🔄 fitting swing-level models …\")\n",
    "mdl_gender = train_swing_target(y_enc[\"gender\"])\n",
    "mdl_hand   = train_swing_target(y_enc[\"hold racket handed\"])\n",
    "mdl_level  = train_swing_target(y_enc[\"level\"],\n",
    "                                num_class=len(enc[\"level\"].classes_))\n",
    "mdl_years  = train_swing_target(y_enc[\"play years\"],\n",
    "                                num_class=len(enc[\"play years\"].classes_),\n",
    "                                params_extra={\"class_weight\":\"balanced\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed7f61a2-2de4-4073-87b4-ce39b6333ae5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f0c8ee0de43457f864750f04ffc4857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Targets:   0%|          | 0/4 [00:00<?, ?target/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de50e3eabe94f529988553994461b0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CatBoost OOF:   0%|          | 0/5 [00:00<?, ?fold/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] CatBoost Fold-1 has only class 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80e622c376c347e388d8fa1eb38ca0b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "XGBoost OOF:   0%|          | 0/5 [00:00<?, ?fold/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] XGBoost Fold-1 has only class 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b535a8e021454333b946de4a8b580dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CatBoost OOF:   0%|          | 0/5 [00:00<?, ?fold/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] CatBoost Fold-0 has only class 0\n",
      "[WARN] CatBoost Fold-3 has only class 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17c5055c3ee942d78acc9533b1eb0b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "XGBoost OOF:   0%|          | 0/5 [00:00<?, ?fold/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] XGBoost Fold-0 has only class 0\n",
      "[WARN] XGBoost Fold-3 has only class 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc38a27d520c4f5fbcaa9520eb694e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CatBoost OOF:   0%|          | 0/5 [00:00<?, ?fold/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c23d9cbdf1242fc93474c7475f229d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "XGBoost OOF:   0%|          | 0/5 [00:00<?, ?fold/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f06290a1e7742ff8673cb320bead598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CatBoost OOF:   0%|          | 0/5 [00:00<?, ?fold/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a417e3e9f9404cb225d0bbfb39f8b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "XGBoost OOF:   0%|          | 0/5 [00:00<?, ?fold/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# >>>>>>   ADD CATBOOST + XGBOOST + WEIGHTED BLEND   <<<<<<\n",
    "# ==============================================================\n",
    "\n",
    "from itertools import product\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import catboost as cb, xgboost as xgb\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ── 1) helpers ───────────────────────────────────────────────────────\n",
    "def _slice_to_present_classes(pred: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Align prediction matrix with the *present* class labels in y.\n",
    "\n",
    "    * Binary target   → return P(class=1) if available, else class=0.\n",
    "    * Single-class    → return the only column\n",
    "    * Multi-class K>2 → keep columns of present classes\n",
    "    \"\"\"\n",
    "    if pred.ndim == 1:            # Already 1-D\n",
    "        return pred\n",
    "\n",
    "    cls = np.unique(y)            # present classes\n",
    "    n_cls = pred.shape[1]         # number of output columns\n",
    "\n",
    "    if len(cls) == 1:\n",
    "        return pred[:, cls[0]][:, None]    # single-class (fold-level)\n",
    "        tqdm.write(f\"[WARN] Only one class in fold: y = {np.unique(y)}\")\n",
    "        \n",
    "    if len(cls) == 2 and n_cls == 2:\n",
    "        return pred[:, 1]                  # standard binary: use P(class=1)\n",
    "\n",
    "    if len(cls) == 2 and n_cls == 1:\n",
    "        return pred[:, 0]                  # fallback: only 1 column predicted\n",
    "\n",
    "    return pred[:, cls]                   # multi-class (subset)\n",
    "\n",
    "def oof_catboost(y, is_mult=False, **cat_kw):\n",
    "    \"\"\"OOF predictions (and fitted models) for CatBoostClassifier.\"\"\"\n",
    "    n_cls = len(np.unique(y)) if is_mult else 1\n",
    "    oof   = np.zeros((len(X_sw), n_cls), dtype=np.float32)\n",
    "    models = []\n",
    "\n",
    "    for i, (tr_idx, vl_idx) in enumerate(tqdm(gkf, desc=\"CatBoost OOF\", unit=\"fold\")):\n",
    "        mdl = cb.CatBoostClassifier(**cat_kw).fit(X_sw.iloc[tr_idx], y[tr_idx])\n",
    "        models.append(mdl)\n",
    "\n",
    "        prob = mdl.predict_proba(X_sw.iloc[vl_idx])\n",
    "        if not is_mult:\n",
    "            prob = prob[:, 1][:, None]       # keep P(class 1) only\n",
    "\n",
    "        oof[vl_idx] = prob\n",
    "        if len(np.unique(y[vl_idx])) == 1:\n",
    "            tqdm.write(f\"[WARN] CatBoost Fold-{i} has only class {np.unique(y[vl_idx])[0]}\")\n",
    "            \n",
    "    auc = roc_auc_score(\n",
    "        y,\n",
    "        _slice_to_present_classes(oof, y),\n",
    "        multi_class=\"ovr\" if is_mult and len(np.unique(y)) > 2 else \"raise\"\n",
    "    )\n",
    "    return oof.squeeze(), models, auc\n",
    "\n",
    "\n",
    "def oof_xgboost(y, num_class=None):\n",
    "    \"\"\"OOF predictions (and fitted models) for XGBoost.\"\"\"\n",
    "    n_cls  = num_class or len(np.unique(y))\n",
    "    oof    = np.zeros((len(X_sw), n_cls), dtype=np.float32)\n",
    "    models = []\n",
    "\n",
    "    for i, (tr_idx, vl_idx) in enumerate(tqdm(gkf, desc=\"XGBoost OOF\", unit=\"fold\")):\n",
    "        trD = xgb.DMatrix(X_sw.iloc[tr_idx], label=y[tr_idx])\n",
    "        vlD = xgb.DMatrix(X_sw.iloc[vl_idx], label=y[vl_idx])\n",
    "\n",
    "        params = dict(\n",
    "            max_depth=7, eta=0.05, subsample=0.8, colsample_bytree=0.8,\n",
    "            seed=42, tree_method=\"hist\",\n",
    "            objective=\"binary:logistic\" if n_cls == 1 else \"multi:softprob\",\n",
    "            eval_metric=\"auc\"          if n_cls == 1 else \"mlogloss\",\n",
    "        )\n",
    "        if n_cls > 1:\n",
    "            params[\"num_class\"] = n_cls\n",
    "\n",
    "        bst = xgb.train(params, trD, num_boost_round=800,\n",
    "                        evals=[(vlD, \"val\")], verbose_eval=False)\n",
    "        models.append(bst)\n",
    "\n",
    "        prob = bst.predict(vlD)\n",
    "        if n_cls == 1:                       # shape (N,) → (N,1)\n",
    "            prob = prob[:, None]\n",
    "\n",
    "        oof[vl_idx] = prob\n",
    "        if len(np.unique(y[vl_idx])) == 1:\n",
    "            tqdm.write(f\"[WARN] XGBoost Fold-{i} has only class {np.unique(y[vl_idx])[0]}\")\n",
    "\n",
    "    auc = roc_auc_score(\n",
    "        y,\n",
    "        _slice_to_present_classes(oof, y),\n",
    "        multi_class=\"ovr\" if n_cls > 2 else \"raise\"\n",
    "    )\n",
    "    return oof.squeeze(), models, auc\n",
    "\n",
    "\n",
    "# ── 2) prepare targets dict ──────────────────────────────────────────\n",
    "targets = {\n",
    "    \"gender\": dict(y=y_enc[\"gender\"],             mult=False),\n",
    "    \"hand\"  : dict(y=y_enc[\"hold racket handed\"], mult=False),\n",
    "    \"level\" : dict(y=y_enc[\"level\"],              mult=True,\n",
    "                   class_cnt=len(enc[\"level\"].classes_)),\n",
    "    \"years\" : dict(y=y_enc[\"play years\"],         mult=True,\n",
    "                   class_cnt=len(enc[\"play years\"].classes_)),\n",
    "}\n",
    "\n",
    "oof  = {\"lgb\": {}, \"cat\": {}, \"xgb\": {}}\n",
    "mods = {\"cat\": {}, \"xgb\": {}}\n",
    "\n",
    "# ── 3) build OOF for every target ────────────────────────────────────\n",
    "for name, cfg in tqdm(targets.items(), desc=\"Targets\", unit=\"target\"):\n",
    "    # ---- LightGBM predictions already trained ----------------------\n",
    "    lgb_pred = (1 - mdl_gender.predict(X_sw) if name == \"gender\" else\n",
    "                1 - mdl_hand.predict(X_sw)   if name == \"hand\"   else\n",
    "                mdl_level.predict(X_sw)      if name == \"level\"  else\n",
    "                mdl_years.predict(X_sw))\n",
    "    oof[\"lgb\"][name] = lgb_pred\n",
    "\n",
    "    # ---- CatBoost --------------------------------------------------\n",
    "    cat_par = dict(\n",
    "        loss_function=\"Logloss\" if not cfg[\"mult\"] else \"MultiClass\",\n",
    "        depth=6, learning_rate=0.05, iterations=800,\n",
    "        random_seed=42, verbose=False\n",
    "    )\n",
    "    oof_cat, mod_cat, _ = oof_catboost(cfg[\"y\"], cfg[\"mult\"], **cat_par)\n",
    "    oof[\"cat\"][name], mods[\"cat\"][name] = oof_cat, mod_cat\n",
    "\n",
    "    # ---- XGBoost ---------------------------------------------------\n",
    "    oof_xgb, mod_xgb, _ = oof_xgboost(cfg[\"y\"], cfg.get(\"class_cnt\"))\n",
    "    oof[\"xgb\"][name], mods[\"xgb\"][name] = oof_xgb, mod_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5fa0404-5a96-443a-81a1-f5bbc83a1451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ef529571e9443e2b442f5e5f019c071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Weight search:   0%|          | 0/4 [00:00<?, ?target/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ac8a04c53b44458bac8b1c34f9488e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/121 [00:00<?, ?comb/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender  best AUC = 0.7585  weights = (np.float64(0.0), np.float64(0.0), np.float64(1.0))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a84fd078345e43809b3516de309c9c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/121 [00:00<?, ?comb/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hand    best AUC = 0.9995  weights = (np.float64(0.0), np.float64(0.2), np.float64(0.8))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07cc58ed3cc947ffbde67d93dd7360ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/121 [00:00<?, ?comb/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level   best AUC = 0.9999  weights = (np.float64(1.0), np.float64(0.0), np.float64(0.0))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03f9d42d2a604e69812bd9362b4ac9eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/121 [00:00<?, ?comb/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "years   best AUC = 0.9992  weights = (np.float64(1.0), np.float64(0.0), np.float64(0.0))\n"
     ]
    }
   ],
   "source": [
    "# ── 4) non-negative weight search (wl + wc + wx = 1) ────────────────\n",
    "best_w = {}\n",
    "\n",
    "for name, cfg in tqdm(targets.items(), desc=\"Weight search\", unit=\"target\"):\n",
    "    y_true = cfg[\"y\"]\n",
    "    mult   = cfg[\"mult\"]\n",
    "    # l, c, x = oof[\"lgb\"][name], oof[\"cat\"][name], oof[\"xgb\"][name]\n",
    "    l = _slice_to_present_classes(oof[\"lgb\"][name], y_true)\n",
    "    c = _slice_to_present_classes(oof[\"cat\"][name], y_true)\n",
    "    x = _slice_to_present_classes(oof[\"xgb\"][name], y_true)\n",
    "\n",
    "    # Correct:\n",
    "    if mult:\n",
    "        scorer = lambda z: roc_auc_score(y_true, z, multi_class=\"ovr\")\n",
    "    else:\n",
    "        scorer = lambda z: roc_auc_score(y_true, z)\n",
    "\n",
    "    top_auc, best_triplet = -1, None\n",
    "    for wl, wc in tqdm(product(np.linspace(0, 1, 11), repeat=2),\n",
    "                       total=121, leave=False, unit=\"comb\"):\n",
    "        if wl + wc > 1:\n",
    "            continue\n",
    "        wx     = 1 - wl - wc\n",
    "        blend  = wl * l + wc * c + wx * x\n",
    "        score  = scorer(_slice_to_present_classes(blend, y_true) if mult else blend)\n",
    "        if score > top_auc:\n",
    "            top_auc, best_triplet = score, (wl, wc, wx)\n",
    "\n",
    "    best_w[name] = best_triplet\n",
    "    print(f\"{name:<6}  best AUC = {top_auc:.4f}  weights = {best_triplet}\")\n",
    "\n",
    "\n",
    "# ── 5) utility helpers for later use ────────────────────────────────\n",
    "def rank01(arr: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Rank-normalize an array to [0,1].\"\"\"\n",
    "    return pd.Series(arr).rank(method=\"average\").to_numpy(dtype=np.float32) / len(arr)\n",
    "\n",
    "\n",
    "def predict_test(model_list, X_df, mult=False):\n",
    "    \"\"\"Average the probability outputs of trained models on X_df.\"\"\"\n",
    "    if isinstance(model_list[0], cb.CatBoostClassifier):\n",
    "        prob = np.mean([m.predict_proba(X_df) for m in model_list], axis=0)\n",
    "    else:                                        # XGBoost\n",
    "        dm   = xgb.DMatrix(X_df)\n",
    "        prob = np.mean([m.predict(dm) for m in model_list], axis=0)\n",
    "    return prob[:, 0] if not mult else prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9b8c04a-65b1-47e6-b0ab-163498f804fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ec9d9e6bb184c148bd41778b8a2e730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "features:   0%|          | 0/1430 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ X_test_sw.shape (session-level): (1430, 522)\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Step 5-S ▸ SESSION-level TEST prediction\n",
    "# ================================\n",
    "TEST_DIR = Path(\"39_Test_Dataset\")\n",
    "test_info = pd.read_csv(TEST_DIR / \"test_info.csv\")\n",
    "test_txts = sorted((TEST_DIR / \"test_data\").glob(\"*.txt\"))\n",
    "\n",
    "# A) 先構造 swing-level 特徵並加上 unique_id，如之前所做\n",
    "sw_rows, uid_list, sid_list = [], [], []\n",
    "\n",
    "for fp in tqdm(test_txts, desc=\"features\"):\n",
    "    uid = int(fp.stem)\n",
    "    trace = load_sensor_data(fp)\n",
    "    swings = segment_27_equal(trace)\n",
    "\n",
    "    mode = test_info.loc[test_info[\"unique_id\"] == uid, \"mode\"].iat[0]\n",
    "\n",
    "    for s_idx, sw in enumerate(swings):\n",
    "        feat = extract_swing_features(sw)\n",
    "        for m in range(1, 11):\n",
    "            feat[f\"mode_{m}\"] = int(mode == m)\n",
    "        sw_rows.append(feat)\n",
    "        uid_list.append(uid)\n",
    "        sid_list.append(s_idx)\n",
    "\n",
    "df_test_swing = pd.DataFrame(sw_rows).astype(np.float32)\n",
    "df_test_swing[\"unique_id\"] = uid_list\n",
    "\n",
    "# B) 聚合為 session-level 特徵，使欄位名稱與訓練端 X_sw 完全一致\n",
    "agg_dict = {col: [\"mean\", \"std\", \"max\", \"min\"] for col in df_test_swing.columns if col != \"unique_id\"}\n",
    "df_test_session = df_test_swing.groupby(\"unique_id\").agg(agg_dict)\n",
    "df_test_session.columns = [f\"{feat}_{stat}27\" for feat, stat in df_test_session.columns]\n",
    "df_test_session = df_test_session.reset_index()\n",
    "\n",
    "# ── Patch: add test-side quantile-pooled features ─────────────────────\n",
    "q_test = build_quantile_pool(df_test_swing)          # <- same helper\n",
    "df_test_session = df_test_session.merge(q_test, on=\"unique_id\")\n",
    "\n",
    "# C) 最終 X_test_sw：去掉 unique_id，並選用與訓練同名的欄位\n",
    "X_test_sw = df_test_session.drop(columns=[\"unique_id\"])[X_sw.columns].astype(np.float32)\n",
    "print(\"✅ X_test_sw.shape (session-level):\", X_test_sw.shape)\n",
    "\n",
    "# Start patch --- Patch catboost + xgb models blending ----------------------------\n",
    "p_test = {\"lgb\":{}, \"cat\":{}, \"xgb\":{}}\n",
    "# LightGBM binary → need to flip: we want P(class 0) = Female / Left hand\n",
    "p_test[\"lgb\"][\"gender\"] = 1 - mdl_gender.predict(X_test_sw)\n",
    "p_test[\"lgb\"][\"hand\"]   = 1 - mdl_hand.predict(X_test_sw)\n",
    "p_test[\"lgb\"][\"level\"]  = mdl_level.predict(X_test_sw)\n",
    "p_test[\"lgb\"][\"years\"]  = mdl_years.predict(X_test_sw)\n",
    "\n",
    "for n,cfg in targets.items():\n",
    "    p_test[\"cat\"][n] = predict_test(mods[\"cat\"][n], X_test_sw, cfg[\"mult\"])\n",
    "    p_test[\"xgb\"][n] = predict_test(mods[\"xgb\"][n], X_test_sw, cfg[\"mult\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c79dd0e-c7ac-4a15-87c3-ce07b67aefef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ wrote submission_GISH_blended.csv\n",
      "✅ wrote submission_GISH_rank.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# D) 預測：mdl_gender、mdl_hand、mdl_level、mdl_years 均已在訓練時使用 session-level 資料 fit\\np_g   = mdl_gender.predict(X_test_sw)      # 長度 = n_sessions\\np_h   = mdl_hand.predict(X_test_sw)\\np_lvl = mdl_level.predict(X_test_sw)       # shape = (n_sessions, K_level)\\np_yrs = mdl_years.predict(X_test_sw)       # shape = (n_sessions, K_years)\\n\\n# E) LightGBM 輸出為 P(class=1)，這裡反向取 P(class=0)\\np_g = 1.0 - p_g    # Gender → P(Female)\\np_h = 1.0 - p_h    # Hand   → P(Left hand)\\n\\n# F) 構造一個字典：unique_id → p_g / p_h\\nsession_ids = df_test_session[\"unique_id\"].tolist()\\ngender_map = dict(zip(session_ids, p_g))\\nhand_map   = dict(zip(session_ids, p_h))\\n\\n# G) multi-class: 對 p_lvl / p_yrs 也直接以 session-level 平均（此時已經是一筆一 session）\\nlvl_cols = [f\"level_{i+2}\" for i in range(p_lvl.shape[1])]\\nyrs_cols = [f\"play years_{i}\" for i in range(p_yrs.shape[1])]\\n\\nlvl_map = {\\n    session_ids[i]: {lvl_cols[j]: p_lvl[i, j] for j in range(p_lvl.shape[1])}\\n    for i in range(len(session_ids))\\n}\\nyrs_map = {\\n    session_ids[i]: {yrs_cols[j]: p_yrs[i, j] for j in range(p_yrs.shape[1])}\\n    for i in range(len(session_ids))\\n}\\n\\n# H) 最終 submission\\nsample = pd.read_csv(TEST_DIR / \"sample_submission.csv\")\\nsubmission = sample.copy()\\n\\nsubmission[\"gender\"]             = submission[\"unique_id\"].map(gender_map)\\nsubmission[\"hold racket handed\"] = submission[\"unique_id\"].map(hand_map)\\n\\n# 填入 multi-class 欄位\\nfor col in lvl_cols:\\n    submission[col] = submission[\"unique_id\"].map(lambda uid: lvl_map.get(uid, {}).get(col, np.nan))\\nfor col in yrs_cols:\\n    submission[col] = submission[\"unique_id\"].map(lambda uid: yrs_map.get(uid, {}).get(col, np.nan))\\n\\nprint(\"✅ FINAL submission prepared\")\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 5)  build blended submission (prob-avg & rank-avg)  -------------\n",
    "sample = pd.read_csv(TEST_DIR/\"sample_submission.csv\")\n",
    "\n",
    "def make_sub(rank=False, fname=\"submission_blended_prob.csv\"):\n",
    "    sub = sample.copy()\n",
    "    for n,cfg in targets.items():\n",
    "        wl,wc,wx = best_w[n]\n",
    "        arrs=[p_test[m][n] for m in (\"lgb\",\"cat\",\"xgb\")]\n",
    "        if rank: arrs=[rank01(a) if a.ndim==1 else np.apply_along_axis(rank01,0,a) for a in arrs]\n",
    "\n",
    "        if cfg[\"mult\"]:\n",
    "            out_cols = ([f\"level_{i+2}\" if n==\"level\"\n",
    "                         else f\"play years_{i}\" for i in range(cfg[\"class_cnt\"])])\n",
    "            blend = wl*arrs[0] + wc*arrs[1] + wx*arrs[2]   # (N,K)\n",
    "            for j,c in enumerate(out_cols): sub[c]=blend[:,j]\n",
    "        else:\n",
    "            col = \"gender\" if n==\"gender\" else \"hold racket handed\"\n",
    "            sub[col] = wl*arrs[0] + wc*arrs[1] + wx*arrs[2]\n",
    "    sub.to_csv(fname,index=False,float_format=\"%.10f\")\n",
    "    print(\"✅ wrote\",fname)\n",
    "\n",
    "make_sub(rank=False, fname=\"submission_GISH_blended.csv\")\n",
    "make_sub(rank=True , fname=\"submission_GISH_rank.csv\")\n",
    "# End patch =============================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72706357-fb77-493f-b971-1c521e6acc6e",
   "metadata": {},
   "source": [
    "# quick sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46905517-c0b8-4ebd-bf56-83e698470cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train / test feature matrices aligned\n",
      "🎉 Both blended submissions look good – ready to upload.\n",
      "gender  LGB AUC  1.0000\n",
      "hand    LGB AUC  0.9999\n",
      "level   LGB AUC  0.9999\n",
      "years   LGB AUC  0.9992\n"
     ]
    }
   ],
   "source": [
    "# 1️⃣ feature columns identical\n",
    "assert set(X_sw.columns) == set(X_test_sw.columns)\n",
    "assert X_sw.shape[1]     == X_test_sw.shape[1]\n",
    "print(\"✅ Train / test feature matrices aligned\")\n",
    "\n",
    "# 2️⃣ probability ranges\n",
    "for f in [\"submission_GISH_blended.csv\", \"submission_GISH_rank.csv\"]:\n",
    "    df = pd.read_csv(f)\n",
    "    assert df.drop(columns=\"unique_id\").apply(lambda col: col.between(0,1)).all().all(), f\"{f} has out-of-range values\"\n",
    "print(\"🎉 Both blended submissions look good – ready to upload.\")\n",
    "\n",
    "# 3️⃣ OOF sanity\n",
    "for t in [\"gender\", \"hand\", \"level\", \"years\"]:\n",
    "    y_t = targets[t][\"y\"]\n",
    "    if targets[t][\"mult\"]:                        # multi-class => nothing to flip\n",
    "        p   = _slice_to_present_classes(oof[\"lgb\"][t], y_t)\n",
    "        auc = roc_auc_score(y_t, p, multi_class=\"ovr\")\n",
    "    else:                                         # binary  -> need P(class 1)\n",
    "        p0  = oof[\"lgb\"][t].ravel()               # P(class 0) that we stored\n",
    "        p1  = 1.0 - p0                            # -> P(class 1)\n",
    "        auc = roc_auc_score(y_t, p1)\n",
    "\n",
    "    print(f\"{t:6}  LGB AUC  {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b242101-28ca-409c-972e-7b97b8b6da73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "── Self-check on training sessions ──\n"
     ]
    }
   ],
   "source": [
    "# ===========================================================\n",
    "# 🔎  FULL-TRAIN SELF-CHECK  ── blended AUC on training set\n",
    "#      (should be almost perfect → pipeline sanity)\n",
    "# ===========================================================\n",
    "print(\"\\n── Self-check on training sessions ──\")\n",
    "\n",
    "# 1) gather per-model train-set predictions\n",
    "train_p = {\"lgb\": {}, \"cat\": {}, \"xgb\": {}}\n",
    "\n",
    "train_p[\"lgb\"][\"gender\"] = 1 - mdl_gender.predict(X_sw)\n",
    "train_p[\"lgb\"][\"hand\"]   = 1 - mdl_hand.predict(X_sw)\n",
    "train_p[\"lgb\"][\"level\"]  = mdl_level.predict(X_sw)\n",
    "train_p[\"lgb\"][\"years\"]  = mdl_years.predict(X_sw)\n",
    "\n",
    "for n, cfg in targets.items():\n",
    "    train_p[\"cat\"][n] = predict_test(mods[\"cat\"][n], X_sw,  cfg[\"mult\"])\n",
    "    train_p[\"xgb\"][n] = predict_test(mods[\"xgb\"][n], X_sw,  cfg[\"mult\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3943c6f4-d5e2-4a7c-acac-40dc545399ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ▸ gender  blended-prob AUC = 1.0000\n",
      "  ▸ hand    blended-prob AUC = 1.0000\n",
      "  ▸ level   blended-prob AUC = 0.9999\n",
      "  ▸ years   blended-prob AUC = 0.9992\n",
      "✅  All AUCs should be ≳ 0.98 — indicates data flow is correct.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2) blend with the previously-searched weights and score AUC\n",
    "for n, cfg in targets.items():\n",
    "    wl, wc, wx = best_w[n]\n",
    "    arrs       = [train_p[m][n] for m in (\"lgb\", \"cat\", \"xgb\")]\n",
    "\n",
    "    blend_prob = wl * arrs[0] + wc * arrs[1] + wx * arrs[2]\n",
    "\n",
    "    if cfg[\"mult\"]:\n",
    "       auc = roc_auc_score(cfg[\"y\"], blend_prob, multi_class=\"ovr\")\n",
    "    else:                              # binary → need P(class 1) again\n",
    "       auc = roc_auc_score(cfg[\"y\"], 1.0 - blend_prob)\n",
    "\n",
    "    print(f\"  ▸ {n:<6}  blended-prob AUC = {auc:6.4f}\")\n",
    "\n",
    "print(\"✅  All AUCs should be ≳ 0.98 — indicates data flow is correct.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6ffd97-af97-49b6-9792-e9cb00d6310d",
   "metadata": {},
   "source": [
    "# files difference check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46d75513-8c06-46b2-b5bb-51d9600a9645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👌 Train/Test feature matrices perfectly aligned.\n"
     ]
    }
   ],
   "source": [
    "# Sanity: ensure identical feature lists and dtypes\n",
    "assert list(X_sw.columns) == list(X_test_sw.columns), \"❌ Column order mismatch!\"\n",
    "assert all(X_sw.dtypes == X_test_sw.dtypes),          \"❌ Dtype mismatch!\"\n",
    "print(\"👌 Train/Test feature matrices perfectly aligned.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71c0e55f-308a-417e-a4fc-9b95214ead34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Largest absolute change per column:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max |Δ|</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>level_4</th>\n",
       "      <td>0.934347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>0.911348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>level_3</th>\n",
       "      <td>0.800034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>play years_0</th>\n",
       "      <td>0.794025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hold racket handed</th>\n",
       "      <td>0.724491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>level_2</th>\n",
       "      <td>0.710902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>play years_2</th>\n",
       "      <td>0.618720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>play years_1</th>\n",
       "      <td>0.458617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>level_5</th>\n",
       "      <td>0.447412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     max |Δ|\n",
       "level_4             0.934347\n",
       "gender              0.911348\n",
       "level_3             0.800034\n",
       "play years_0        0.794025\n",
       "hold racket handed  0.724491\n",
       "level_2             0.710902\n",
       "play years_2        0.618720\n",
       "play years_1        0.458617\n",
       "level_5             0.447412"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall average change: 0.2997817779362782\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "\n",
    "sub_v2 = pd.read_csv(\"submission_kueipo.csv\").set_index(\"unique_id\")\n",
    "sub_v3 = pd.read_csv(\"submission_GISH_rank.csv\").set_index(\"unique_id\")\n",
    "\n",
    "# # 2️⃣  probability range check for v3\n",
    "# outside = ~sub_v3.between(0,1).all().all()\n",
    "# print(\"✅ all probs in [0,1]\" if not outside else \"⚠️ some probs out of bounds\")\n",
    "\n",
    "# 3️⃣  compare v2 vs v3\n",
    "diff = (sub_v3 - sub_v2).abs()\n",
    "print(\"\\nLargest absolute change per column:\")\n",
    "display(diff.max().sort_values(ascending=False).to_frame(\"max |Δ|\"))\n",
    "\n",
    "print(\"\\nOverall average change:\", diff.values.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464c3955-f49a-498e-b32b-d81eee389f65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
